# Social Media Analytics Using COVID-19 Dataset

## Overview

This project presents a **big data–driven analytical framework** for large-scale analysis of over **one million COVID-19–related tweets**. It applies advanced **natural language processing (NLP)** techniques to transform semi-structured social media data into actionable insights that can support **public health monitoring, situational awareness, and policy decision-making**.

The framework is designed to operate at scale and emphasizes automation, reproducibility, and minimal reliance on manual labeling. By combining transformer-based language models with unsupervised and zero-shot learning techniques, the system enables nuanced analysis of public discourse during different phases of the COVID-19 pandemic.

---

## Repository Structure

```bash
├── outputs/ # Generated analysis outputs
├── .gitignore
├── Cloud_and_big_data_report.pdf # Project report detailing methodology and results
├── covid.yml # Conda environment configuration
├── dataset.sh # Script to download and prepare the dataset
├── main.py # Primary analytics pipeline
├── new_main.py # Updated or alternative pipeline implementation
├── pipeline.log # Sample pipeline execution log
├── pipeline_corrected.log # Corrected execution log
└── README.md

```
---

## Core Analytical Pipeline

The analytics framework is structured around four complementary analytical axes:

### Granular Topic Modeling
High-dimensional transformer embeddings generated using **all-MiniLM-L6-v2** are clustered with **HDBSCAN** and analyzed through the **BERTopic** framework. This approach enables the discovery of fine-grained and evolving discussion themes without requiring predefined topic labels.

### Dual-Layer Affective Estimation
Pre-trained transformer models are used to perform both **sentiment polarity classification** (positive, negative, neutral) and **emotion-level inference** (e.g., joy, anger, fear). This dual-layer approach captures both general sentiment trends and more nuanced emotional dynamics within social media discourse.

### Unsupervised Anomaly Detection
The framework applies the **Isolation Forest** algorithm to identify statistically anomalous tweets. This helps surface rare, high-impact events, potential misinformation, or abnormal communication patterns that deviate from typical discourse.

### Bot Identification
To reduce the influence of synthetic amplification, the pipeline integrates **Botometer-based bot scoring**. Accounts exhibiting automated behavior can be identified and filtered, ensuring that analytical insights primarily reflect authentic human engagement.

---

## Dataset Overview

The analysis is conducted on a global corpus of **English-language tweets** collected across three key phases of the COVID-19 pandemic:

- **Early Response Phase (April–June 2020):** Approximately 235,000 tweets capturing initial public reactions and uncertainty  
- **Peak Fatality Phase (August–October 2020):** Approximately 320,000 tweets reflecting heightened concern and discourse around case surges  
- **Vaccine Deployment Phase (April–June 2021):** Approximately 489,000 tweets focused on vaccination rollout, hesitancy, and public response  

Together, these phases enable longitudinal analysis of how public sentiment, topics, and behaviors evolved throughout the pandemic.

---

## Dataset

The project uses a COVID-19 related social media dataset. The dataset must be downloaded and prepared before running the analytics pipeline.

To download and set up the dataset, run:

```bash
bash dataset.sh

```
---
## Environment Setup

The project uses a Conda environment defined in covid.yml.

Create and activate the environment

```bash
conda env create -f covid.yml
conda activate COVID

pip install -r requirements.txt
```
---
## Usage

After setting up the environment and downloading the dataset, run the analytics pipeline:

```bash
python main.py
```

or the updated script

```bash
python new_main.py
```
---

## Project Workflow

Data Acquisition : The dataset is downloaded and prepared using dataset.sh.

Data Preprocessing: Raw data is cleaned, structured, and validated for analysis.

Analytics and Transformation: The pipeline performs analytical transformations to identify patterns and trends within the dataset.

Output Generation: Processed results, visualizations, and summaries are saved to the outputs/ directory.

Reporting: Findings and methodology are documented in Cloud_and_big_data_report.pdf.

---
## Outputs

The outputs/ directory contains results generated by the pipeline, which may include:

Aggregated data files
Visualizations and plots
Summary statistics and analytical reports

These outputs help illustrate insights derived from COVID-19 related social media activity.

---
