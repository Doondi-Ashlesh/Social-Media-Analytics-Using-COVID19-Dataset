# Social Media Analytics Using COVID-19 Dataset

## Overview

This project presents a big data-driven framework for the large-scale analysis of over one million COVID-19-related tweets (pp. 1, 13). It utilizes advanced natural language processing (NLP) to transform semi-structured social media data into actionable insights for public health monitoring and policy formulation (pp. 1, 3).
ğŸ› ï¸ Core Analytical Pipeline
The framework integrates four major unsupervised and zero-shot analytical axes (p. 11):
Granular Topic Modeling: Uses HDBSCAN clustering on high-dimensional transformer embeddings (via all-MiniLM-L6-v2) and the BERTopic framework to identify nuanced subtopics (pp. 11, 16).
Dual-Layer Affective Estimation: Employs pre-trained transformer models to automate sentiment polarity (Positive, Negative, Neutral) and granular emotion classification (e.g., joy, anger, fear) without manual labeling (pp. 12, 16).
Unsupervised Anomaly Detection: Implements the Isolation Forest algorithm to detect statistically deviant content, flagging potential misinformation, spam, or rare high-impact events (pp. 12, 18).
Bot Identification: Leverages the Botometer model to assign "Bot Scores" to accounts, enabling the filtration of synthetic amplification to ensure findings reflect genuine human discourse (pp. 12, 20).
ğŸ“Š Dataset Overview
The analysis is conducted on a global corpus of English-language tweets collected across three critical pandemic phases between April 2020 and June 2021 (pp. 12-13):
Phase 1 (Early Response): ~235k tweets (Aprilâ€“June 2020) (p. 13).
Phase 2 (Peak Fatality): ~320k tweets (Augustâ€“October 2020) (p. 13).
Phase 3 (Vaccine Deployment): ~489k tweets (Aprilâ€“June 2021) (p. 13).


---

## Repository Structure

```bash
â”œâ”€â”€ outputs/ # Generated analysis outputs
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Cloud_and_big_data_report.pdf # Project report detailing methodology and results
â”œâ”€â”€ covid.yml # Conda environment configuration
â”œâ”€â”€ dataset.sh # Script to download and prepare the dataset
â”œâ”€â”€ main.py # Primary analytics pipeline
â”œâ”€â”€ new_main.py # Updated or alternative pipeline implementation
â”œâ”€â”€ pipeline.log # Sample pipeline execution log
â”œâ”€â”€ pipeline_corrected.log # Corrected execution log
â””â”€â”€ README.md

```
---

## Dataset

The project uses a COVID-19 related social media dataset. The dataset must be downloaded and prepared before running the analytics pipeline.

To download and set up the dataset, run:

```bash
bash dataset.sh

```
---
## Environment Setup

The project uses a Conda environment defined in covid.yml.

Create and activate the environment

```bash
conda env create -f covid.yml
conda activate COVID

pip install -r requirements.txt
```
---
## Usage

After setting up the environment and downloading the dataset, run the analytics pipeline:

```bash
python main.py
```

or the updated script

```bash
python new_main.py
```
---

## Project Workflow

Data Acquisition : The dataset is downloaded and prepared using dataset.sh.

Data Preprocessing: Raw data is cleaned, structured, and validated for analysis.

Analytics and Transformation: The pipeline performs analytical transformations to identify patterns and trends within the dataset.

Output Generation: Processed results, visualizations, and summaries are saved to the outputs/ directory.

Reporting: Findings and methodology are documented in Cloud_and_big_data_report.pdf.

---
## Outputs

The outputs/ directory contains results generated by the pipeline, which may include:

Aggregated data files
Visualizations and plots
Summary statistics and analytical reports

These outputs help illustrate insights derived from COVID-19 related social media activity.

---
